Launching NeuroLander (Self-Healing Demo)...
[LlamaCpp] Loading model: /usr/local/google/home/antoniopaulino/dev/git/neuroJIT/tensorlang/runtime/models/qwen2.5-coder-7b-instruct-q4_k_m.gguf
llama_model_loader: loaded meta data with 29 key-value pairs and 339 tensors from /usr/local/google/home/antoniopaulino/dev/git/neuroJIT/tensorlang/runtime/models/qwen2.5-coder-7b-instruct-q4_k_m.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct GGUF
llama_model_loader: - kv   3:                           general.finetune str              = Instruct-GGUF
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   7:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   8:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv   9:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  10:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  11:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  12:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                          general.file_type u32              = 15
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - kv  26:                                   split.no u16              = 0
llama_model_loader: - kv  27:                                split.count u16              = 0
llama_model_loader: - kv  28:                        split.tensors.count i32              = 339
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: 0 unused tokens
load: control token: 151660 '<|fim_middle|>' is not marked as EOG
load: control token: 151659 '<|fim_prefix|>' is not marked as EOG
load: control token: 151653 '<|vision_end|>' is not marked as EOG
load: control token: 151648 '<|box_start|>' is not marked as EOG
load: control token: 151646 '<|object_ref_start|>' is not marked as EOG
load: control token: 151649 '<|box_end|>' is not marked as EOG
load: control-looking token: 128247 '</s>' was not control-type; this is probably a bug in the model. its type will be overridden
load: control token: 151655 '<|image_pad|>' is not marked as EOG
load: control token: 151651 '<|quad_end|>' is not marked as EOG
load: control token: 151647 '<|object_ref_end|>' is not marked as EOG
load: control token: 151652 '<|vision_start|>' is not marked as EOG
load: control token: 151654 '<|vision_pad|>' is not marked as EOG
load: control token: 151656 '<|video_pad|>' is not marked as EOG
load: control token: 151644 '<|im_start|>' is not marked as EOG
load: control token: 151661 '<|fim_suffix|>' is not marked as EOG
load: control token: 151650 '<|quad_start|>' is not marked as EOG
load: printing all EOG tokens:
load:   - 128247 ('</s>')
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 23
load: token to piece cache size = 0.9310 MB
print_info: arch                  = qwen2
print_info: vocab_only            = 0
print_info: no_alloc              = 0
print_info: n_ctx_train           = 131072
print_info: n_embd                = 3584
print_info: n_embd_inp            = 3584
print_info: n_layer               = 28
print_info: n_head                = 28
print_info: n_head_kv             = 4
print_info: n_rot                 = 128
print_info: n_swa                 = 0
print_info: is_swa_any            = 0
print_info: n_embd_head_k         = 128
print_info: n_embd_head_v         = 128
print_info: n_gqa                 = 7
print_info: n_embd_k_gqa          = 512
print_info: n_embd_v_gqa          = 512
print_info: f_norm_eps            = 0.0e+00
print_info: f_norm_rms_eps        = 1.0e-06
print_info: f_clamp_kqv           = 0.0e+00
print_info: f_max_alibi_bias      = 0.0e+00
print_info: f_logit_scale         = 0.0e+00
print_info: f_attn_scale          = 0.0e+00
print_info: n_ff                  = 18944
print_info: n_expert              = 0
print_info: n_expert_used         = 0
print_info: n_expert_groups       = 0
print_info: n_group_used          = 0
print_info: causal attn           = 1
print_info: pooling type          = -1
print_info: rope type             = 2
print_info: rope scaling          = linear
print_info: freq_base_train       = 1000000.0
print_info: freq_scale_train      = 1
print_info: n_ctx_orig_yarn       = 131072
print_info: rope_yarn_log_mul     = 0.0000
print_info: rope_finetuned        = unknown
print_info: model type            = 7B
print_info: model params          = 7.62 B
print_info: general.name          = Qwen2.5 Coder 7B Instruct GGUF
print_info: vocab type            = BPE
print_info: n_vocab               = 152064
print_info: n_merges              = 151387
print_info: BOS token             = 151643 '<|endoftext|>'
print_info: EOS token             = 151645 '<|im_end|>'
print_info: EOT token             = 151645 '<|im_end|>'
print_info: PAD token             = 151643 '<|endoftext|>'
print_info: LF token              = 198 'Ċ'
print_info: FIM PRE token         = 151659 '<|fim_prefix|>'
print_info: FIM SUF token         = 151661 '<|fim_suffix|>'
print_info: FIM MID token         = 151660 '<|fim_middle|>'
print_info: FIM PAD token         = 151662 '<|fim_pad|>'
print_info: FIM REP token         = 151663 '<|repo_name|>'
print_info: FIM SEP token         = 151664 '<|file_sep|>'
print_info: EOG token             = 128247 '</s>'
print_info: EOG token             = 151643 '<|endoftext|>'
print_info: EOG token             = 151645 '<|im_end|>'
print_info: EOG token             = 151662 '<|fim_pad|>'
print_info: EOG token             = 151663 '<|repo_name|>'
print_info: EOG token             = 151664 '<|file_sep|>'
print_info: max token length      = 256
load_tensors: loading model tensors, this can take a while... (mmap = true, direct_io = false)
load_tensors: layer   0 assigned to device CPU, is_swa = 0
load_tensors: layer   1 assigned to device CPU, is_swa = 0
load_tensors: layer   2 assigned to device CPU, is_swa = 0
load_tensors: layer   3 assigned to device CPU, is_swa = 0
load_tensors: layer   4 assigned to device CPU, is_swa = 0
load_tensors: layer   5 assigned to device CPU, is_swa = 0
load_tensors: layer   6 assigned to device CPU, is_swa = 0
load_tensors: layer   7 assigned to device CPU, is_swa = 0
load_tensors: layer   8 assigned to device CPU, is_swa = 0
load_tensors: layer   9 assigned to device CPU, is_swa = 0
load_tensors: layer  10 assigned to device CPU, is_swa = 0
load_tensors: layer  11 assigned to device CPU, is_swa = 0
load_tensors: layer  12 assigned to device CPU, is_swa = 0
load_tensors: layer  13 assigned to device CPU, is_swa = 0
load_tensors: layer  14 assigned to device CPU, is_swa = 0
load_tensors: layer  15 assigned to device CPU, is_swa = 0
load_tensors: layer  16 assigned to device CPU, is_swa = 0
load_tensors: layer  17 assigned to device CPU, is_swa = 0
load_tensors: layer  18 assigned to device CPU, is_swa = 0
load_tensors: layer  19 assigned to device CPU, is_swa = 0
load_tensors: layer  20 assigned to device CPU, is_swa = 0
load_tensors: layer  21 assigned to device CPU, is_swa = 0
load_tensors: layer  22 assigned to device CPU, is_swa = 0
load_tensors: layer  23 assigned to device CPU, is_swa = 0
load_tensors: layer  24 assigned to device CPU, is_swa = 0
load_tensors: layer  25 assigned to device CPU, is_swa = 0
load_tensors: layer  26 assigned to device CPU, is_swa = 0
load_tensors: layer  27 assigned to device CPU, is_swa = 0
load_tensors: layer  28 assigned to device CPU, is_swa = 0
create_tensor: loading tensor token_embd.weight
create_tensor: loading tensor output_norm.weight
create_tensor: loading tensor output.weight
create_tensor: loading tensor blk.0.attn_norm.weight
create_tensor: loading tensor blk.0.attn_q.weight
create_tensor: loading tensor blk.0.attn_k.weight
create_tensor: loading tensor blk.0.attn_v.weight
create_tensor: loading tensor blk.0.attn_output.weight
create_tensor: loading tensor blk.0.attn_q.bias
create_tensor: loading tensor blk.0.attn_k.bias
create_tensor: loading tensor blk.0.attn_v.bias
create_tensor: loading tensor blk.0.ffn_norm.weight
create_tensor: loading tensor blk.0.ffn_gate.weight
create_tensor: loading tensor blk.0.ffn_down.weight
create_tensor: loading tensor blk.0.ffn_up.weight
create_tensor: loading tensor blk.1.attn_norm.weight
create_tensor: loading tensor blk.1.attn_q.weight
create_tensor: loading tensor blk.1.attn_k.weight
create_tensor: loading tensor blk.1.attn_v.weight
create_tensor: loading tensor blk.1.attn_output.weight
create_tensor: loading tensor blk.1.attn_q.bias
create_tensor: loading tensor blk.1.attn_k.bias
create_tensor: loading tensor blk.1.attn_v.bias
create_tensor: loading tensor blk.1.ffn_norm.weight
create_tensor: loading tensor blk.1.ffn_gate.weight
create_tensor: loading tensor blk.1.ffn_down.weight
create_tensor: loading tensor blk.1.ffn_up.weight
create_tensor: loading tensor blk.2.attn_norm.weight
create_tensor: loading tensor blk.2.attn_q.weight
create_tensor: loading tensor blk.2.attn_k.weight
create_tensor: loading tensor blk.2.attn_v.weight
create_tensor: loading tensor blk.2.attn_output.weight
create_tensor: loading tensor blk.2.attn_q.bias
create_tensor: loading tensor blk.2.attn_k.bias
create_tensor: loading tensor blk.2.attn_v.bias
create_tensor: loading tensor blk.2.ffn_norm.weight
create_tensor: loading tensor blk.2.ffn_gate.weight
create_tensor: loading tensor blk.2.ffn_down.weight
create_tensor: loading tensor blk.2.ffn_up.weight
create_tensor: loading tensor blk.3.attn_norm.weight
create_tensor: loading tensor blk.3.attn_q.weight
create_tensor: loading tensor blk.3.attn_k.weight
create_tensor: loading tensor blk.3.attn_v.weight
create_tensor: loading tensor blk.3.attn_output.weight
create_tensor: loading tensor blk.3.attn_q.bias
create_tensor: loading tensor blk.3.attn_k.bias
create_tensor: loading tensor blk.3.attn_v.bias
create_tensor: loading tensor blk.3.ffn_norm.weight
create_tensor: loading tensor blk.3.ffn_gate.weight
create_tensor: loading tensor blk.3.ffn_down.weight
create_tensor: loading tensor blk.3.ffn_up.weight
create_tensor: loading tensor blk.4.attn_norm.weight
create_tensor: loading tensor blk.4.attn_q.weight
create_tensor: loading tensor blk.4.attn_k.weight
create_tensor: loading tensor blk.4.attn_v.weight
create_tensor: loading tensor blk.4.attn_output.weight
create_tensor: loading tensor blk.4.attn_q.bias
create_tensor: loading tensor blk.4.attn_k.bias
create_tensor: loading tensor blk.4.attn_v.bias
create_tensor: loading tensor blk.4.ffn_norm.weight
create_tensor: loading tensor blk.4.ffn_gate.weight
create_tensor: loading tensor blk.4.ffn_down.weight
create_tensor: loading tensor blk.4.ffn_up.weight
create_tensor: loading tensor blk.5.attn_norm.weight
create_tensor: loading tensor blk.5.attn_q.weight
create_tensor: loading tensor blk.5.attn_k.weight
create_tensor: loading tensor blk.5.attn_v.weight
create_tensor: loading tensor blk.5.attn_output.weight
create_tensor: loading tensor blk.5.attn_q.bias
create_tensor: loading tensor blk.5.attn_k.bias
create_tensor: loading tensor blk.5.attn_v.bias
create_tensor: loading tensor blk.5.ffn_norm.weight
create_tensor: loading tensor blk.5.ffn_gate.weight
create_tensor: loading tensor blk.5.ffn_down.weight
create_tensor: loading tensor blk.5.ffn_up.weight
create_tensor: loading tensor blk.6.attn_norm.weight
create_tensor: loading tensor blk.6.attn_q.weight
create_tensor: loading tensor blk.6.attn_k.weight
create_tensor: loading tensor blk.6.attn_v.weight
create_tensor: loading tensor blk.6.attn_output.weight
create_tensor: loading tensor blk.6.attn_q.bias
create_tensor: loading tensor blk.6.attn_k.bias
create_tensor: loading tensor blk.6.attn_v.bias
create_tensor: loading tensor blk.6.ffn_norm.weight
create_tensor: loading tensor blk.6.ffn_gate.weight
create_tensor: loading tensor blk.6.ffn_down.weight
create_tensor: loading tensor blk.6.ffn_up.weight
create_tensor: loading tensor blk.7.attn_norm.weight
create_tensor: loading tensor blk.7.attn_q.weight
create_tensor: loading tensor blk.7.attn_k.weight
create_tensor: loading tensor blk.7.attn_v.weight
create_tensor: loading tensor blk.7.attn_output.weight
create_tensor: loading tensor blk.7.attn_q.bias
create_tensor: loading tensor blk.7.attn_k.bias
create_tensor: loading tensor blk.7.attn_v.bias
create_tensor: loading tensor blk.7.ffn_norm.weight
create_tensor: loading tensor blk.7.ffn_gate.weight
create_tensor: loading tensor blk.7.ffn_down.weight
create_tensor: loading tensor blk.7.ffn_up.weight
create_tensor: loading tensor blk.8.attn_norm.weight
create_tensor: loading tensor blk.8.attn_q.weight
create_tensor: loading tensor blk.8.attn_k.weight
create_tensor: loading tensor blk.8.attn_v.weight
create_tensor: loading tensor blk.8.attn_output.weight
create_tensor: loading tensor blk.8.attn_q.bias
create_tensor: loading tensor blk.8.attn_k.bias
create_tensor: loading tensor blk.8.attn_v.bias
create_tensor: loading tensor blk.8.ffn_norm.weight
create_tensor: loading tensor blk.8.ffn_gate.weight
create_tensor: loading tensor blk.8.ffn_down.weight
create_tensor: loading tensor blk.8.ffn_up.weight
create_tensor: loading tensor blk.9.attn_norm.weight
create_tensor: loading tensor blk.9.attn_q.weight
create_tensor: loading tensor blk.9.attn_k.weight
create_tensor: loading tensor blk.9.attn_v.weight
create_tensor: loading tensor blk.9.attn_output.weight
create_tensor: loading tensor blk.9.attn_q.bias
create_tensor: loading tensor blk.9.attn_k.bias
create_tensor: loading tensor blk.9.attn_v.bias
create_tensor: loading tensor blk.9.ffn_norm.weight
create_tensor: loading tensor blk.9.ffn_gate.weight
create_tensor: loading tensor blk.9.ffn_down.weight
create_tensor: loading tensor blk.9.ffn_up.weight
create_tensor: loading tensor blk.10.attn_norm.weight
create_tensor: loading tensor blk.10.attn_q.weight
create_tensor: loading tensor blk.10.attn_k.weight
create_tensor: loading tensor blk.10.attn_v.weight
create_tensor: loading tensor blk.10.attn_output.weight
create_tensor: loading tensor blk.10.attn_q.bias
create_tensor: loading tensor blk.10.attn_k.bias
create_tensor: loading tensor blk.10.attn_v.bias
create_tensor: loading tensor blk.10.ffn_norm.weight
create_tensor: loading tensor blk.10.ffn_gate.weight
create_tensor: loading tensor blk.10.ffn_down.weight
create_tensor: loading tensor blk.10.ffn_up.weight
create_tensor: loading tensor blk.11.attn_norm.weight
create_tensor: loading tensor blk.11.attn_q.weight
create_tensor: loading tensor blk.11.attn_k.weight
create_tensor: loading tensor blk.11.attn_v.weight
create_tensor: loading tensor blk.11.attn_output.weight
create_tensor: loading tensor blk.11.attn_q.bias
create_tensor: loading tensor blk.11.attn_k.bias
create_tensor: loading tensor blk.11.attn_v.bias
create_tensor: loading tensor blk.11.ffn_norm.weight
create_tensor: loading tensor blk.11.ffn_gate.weight
create_tensor: loading tensor blk.11.ffn_down.weight
create_tensor: loading tensor blk.11.ffn_up.weight
create_tensor: loading tensor blk.12.attn_norm.weight
create_tensor: loading tensor blk.12.attn_q.weight
create_tensor: loading tensor blk.12.attn_k.weight
create_tensor: loading tensor blk.12.attn_v.weight
create_tensor: loading tensor blk.12.attn_output.weight
create_tensor: loading tensor blk.12.attn_q.bias
create_tensor: loading tensor blk.12.attn_k.bias
create_tensor: loading tensor blk.12.attn_v.bias
create_tensor: loading tensor blk.12.ffn_norm.weight
create_tensor: loading tensor blk.12.ffn_gate.weight
create_tensor: loading tensor blk.12.ffn_down.weight
create_tensor: loading tensor blk.12.ffn_up.weight
create_tensor: loading tensor blk.13.attn_norm.weight
create_tensor: loading tensor blk.13.attn_q.weight
create_tensor: loading tensor blk.13.attn_k.weight
create_tensor: loading tensor blk.13.attn_v.weight
create_tensor: loading tensor blk.13.attn_output.weight
create_tensor: loading tensor blk.13.attn_q.bias
create_tensor: loading tensor blk.13.attn_k.bias
create_tensor: loading tensor blk.13.attn_v.bias
create_tensor: loading tensor blk.13.ffn_norm.weight
create_tensor: loading tensor blk.13.ffn_gate.weight
create_tensor: loading tensor blk.13.ffn_down.weight
create_tensor: loading tensor blk.13.ffn_up.weight
create_tensor: loading tensor blk.14.attn_norm.weight
create_tensor: loading tensor blk.14.attn_q.weight
create_tensor: loading tensor blk.14.attn_k.weight
create_tensor: loading tensor blk.14.attn_v.weight
create_tensor: loading tensor blk.14.attn_output.weight
create_tensor: loading tensor blk.14.attn_q.bias
create_tensor: loading tensor blk.14.attn_k.bias
create_tensor: loading tensor blk.14.attn_v.bias
create_tensor: loading tensor blk.14.ffn_norm.weight
create_tensor: loading tensor blk.14.ffn_gate.weight
create_tensor: loading tensor blk.14.ffn_down.weight
create_tensor: loading tensor blk.14.ffn_up.weight
create_tensor: loading tensor blk.15.attn_norm.weight
create_tensor: loading tensor blk.15.attn_q.weight
create_tensor: loading tensor blk.15.attn_k.weight
create_tensor: loading tensor blk.15.attn_v.weight
create_tensor: loading tensor blk.15.attn_output.weight
create_tensor: loading tensor blk.15.attn_q.bias
create_tensor: loading tensor blk.15.attn_k.bias
create_tensor: loading tensor blk.15.attn_v.bias
create_tensor: loading tensor blk.15.ffn_norm.weight
create_tensor: loading tensor blk.15.ffn_gate.weight
create_tensor: loading tensor blk.15.ffn_down.weight
create_tensor: loading tensor blk.15.ffn_up.weight
create_tensor: loading tensor blk.16.attn_norm.weight
create_tensor: loading tensor blk.16.attn_q.weight
create_tensor: loading tensor blk.16.attn_k.weight
create_tensor: loading tensor blk.16.attn_v.weight
create_tensor: loading tensor blk.16.attn_output.weight
create_tensor: loading tensor blk.16.attn_q.bias
create_tensor: loading tensor blk.16.attn_k.bias
create_tensor: loading tensor blk.16.attn_v.bias
create_tensor: loading tensor blk.16.ffn_norm.weight
create_tensor: loading tensor blk.16.ffn_gate.weight
create_tensor: loading tensor blk.16.ffn_down.weight
create_tensor: loading tensor blk.16.ffn_up.weight
create_tensor: loading tensor blk.17.attn_norm.weight
create_tensor: loading tensor blk.17.attn_q.weight
create_tensor: loading tensor blk.17.attn_k.weight
create_tensor: loading tensor blk.17.attn_v.weight
create_tensor: loading tensor blk.17.attn_output.weight
create_tensor: loading tensor blk.17.attn_q.bias
create_tensor: loading tensor blk.17.attn_k.bias
create_tensor: loading tensor blk.17.attn_v.bias
create_tensor: loading tensor blk.17.ffn_norm.weight
create_tensor: loading tensor blk.17.ffn_gate.weight
create_tensor: loading tensor blk.17.ffn_down.weight
create_tensor: loading tensor blk.17.ffn_up.weight
create_tensor: loading tensor blk.18.attn_norm.weight
create_tensor: loading tensor blk.18.attn_q.weight
create_tensor: loading tensor blk.18.attn_k.weight
create_tensor: loading tensor blk.18.attn_v.weight
create_tensor: loading tensor blk.18.attn_output.weight
create_tensor: loading tensor blk.18.attn_q.bias
create_tensor: loading tensor blk.18.attn_k.bias
create_tensor: loading tensor blk.18.attn_v.bias
create_tensor: loading tensor blk.18.ffn_norm.weight
create_tensor: loading tensor blk.18.ffn_gate.weight
create_tensor: loading tensor blk.18.ffn_down.weight
create_tensor: loading tensor blk.18.ffn_up.weight
create_tensor: loading tensor blk.19.attn_norm.weight
create_tensor: loading tensor blk.19.attn_q.weight
create_tensor: loading tensor blk.19.attn_k.weight
create_tensor: loading tensor blk.19.attn_v.weight
create_tensor: loading tensor blk.19.attn_output.weight
create_tensor: loading tensor blk.19.attn_q.bias
create_tensor: loading tensor blk.19.attn_k.bias
create_tensor: loading tensor blk.19.attn_v.bias
create_tensor: loading tensor blk.19.ffn_norm.weight
create_tensor: loading tensor blk.19.ffn_gate.weight
create_tensor: loading tensor blk.19.ffn_down.weight
create_tensor: loading tensor blk.19.ffn_up.weight
create_tensor: loading tensor blk.20.attn_norm.weight
create_tensor: loading tensor blk.20.attn_q.weight
create_tensor: loading tensor blk.20.attn_k.weight
create_tensor: loading tensor blk.20.attn_v.weight
create_tensor: loading tensor blk.20.attn_output.weight
create_tensor: loading tensor blk.20.attn_q.bias
create_tensor: loading tensor blk.20.attn_k.bias
create_tensor: loading tensor blk.20.attn_v.bias
create_tensor: loading tensor blk.20.ffn_norm.weight
create_tensor: loading tensor blk.20.ffn_gate.weight
create_tensor: loading tensor blk.20.ffn_down.weight
create_tensor: loading tensor blk.20.ffn_up.weight
create_tensor: loading tensor blk.21.attn_norm.weight
create_tensor: loading tensor blk.21.attn_q.weight
create_tensor: loading tensor blk.21.attn_k.weight
create_tensor: loading tensor blk.21.attn_v.weight
create_tensor: loading tensor blk.21.attn_output.weight
create_tensor: loading tensor blk.21.attn_q.bias
create_tensor: loading tensor blk.21.attn_k.bias
create_tensor: loading tensor blk.21.attn_v.bias
create_tensor: loading tensor blk.21.ffn_norm.weight
create_tensor: loading tensor blk.21.ffn_gate.weight
create_tensor: loading tensor blk.21.ffn_down.weight
create_tensor: loading tensor blk.21.ffn_up.weight
create_tensor: loading tensor blk.22.attn_norm.weight
create_tensor: loading tensor blk.22.attn_q.weight
create_tensor: loading tensor blk.22.attn_k.weight
create_tensor: loading tensor blk.22.attn_v.weight
create_tensor: loading tensor blk.22.attn_output.weight
create_tensor: loading tensor blk.22.attn_q.bias
create_tensor: loading tensor blk.22.attn_k.bias
create_tensor: loading tensor blk.22.attn_v.bias
create_tensor: loading tensor blk.22.ffn_norm.weight
create_tensor: loading tensor blk.22.ffn_gate.weight
create_tensor: loading tensor blk.22.ffn_down.weight
create_tensor: loading tensor blk.22.ffn_up.weight
create_tensor: loading tensor blk.23.attn_norm.weight
create_tensor: loading tensor blk.23.attn_q.weight
create_tensor: loading tensor blk.23.attn_k.weight
create_tensor: loading tensor blk.23.attn_v.weight
create_tensor: loading tensor blk.23.attn_output.weight
create_tensor: loading tensor blk.23.attn_q.bias
create_tensor: loading tensor blk.23.attn_k.bias
create_tensor: loading tensor blk.23.attn_v.bias
create_tensor: loading tensor blk.23.ffn_norm.weight
create_tensor: loading tensor blk.23.ffn_gate.weight
create_tensor: loading tensor blk.23.ffn_down.weight
create_tensor: loading tensor blk.23.ffn_up.weight
create_tensor: loading tensor blk.24.attn_norm.weight
create_tensor: loading tensor blk.24.attn_q.weight
create_tensor: loading tensor blk.24.attn_k.weight
create_tensor: loading tensor blk.24.attn_v.weight
create_tensor: loading tensor blk.24.attn_output.weight
create_tensor: loading tensor blk.24.attn_q.bias
create_tensor: loading tensor blk.24.attn_k.bias
create_tensor: loading tensor blk.24.attn_v.bias
create_tensor: loading tensor blk.24.ffn_norm.weight
create_tensor: loading tensor blk.24.ffn_gate.weight
create_tensor: loading tensor blk.24.ffn_down.weight
create_tensor: loading tensor blk.24.ffn_up.weight
create_tensor: loading tensor blk.25.attn_norm.weight
create_tensor: loading tensor blk.25.attn_q.weight
create_tensor: loading tensor blk.25.attn_k.weight
create_tensor: loading tensor blk.25.attn_v.weight
create_tensor: loading tensor blk.25.attn_output.weight
create_tensor: loading tensor blk.25.attn_q.bias
create_tensor: loading tensor blk.25.attn_k.bias
create_tensor: loading tensor blk.25.attn_v.bias
create_tensor: loading tensor blk.25.ffn_norm.weight
create_tensor: loading tensor blk.25.ffn_gate.weight
create_tensor: loading tensor blk.25.ffn_down.weight
create_tensor: loading tensor blk.25.ffn_up.weight
create_tensor: loading tensor blk.26.attn_norm.weight
create_tensor: loading tensor blk.26.attn_q.weight
create_tensor: loading tensor blk.26.attn_k.weight
create_tensor: loading tensor blk.26.attn_v.weight
create_tensor: loading tensor blk.26.attn_output.weight
create_tensor: loading tensor blk.26.attn_q.bias
create_tensor: loading tensor blk.26.attn_k.bias
create_tensor: loading tensor blk.26.attn_v.bias
create_tensor: loading tensor blk.26.ffn_norm.weight
create_tensor: loading tensor blk.26.ffn_gate.weight
create_tensor: loading tensor blk.26.ffn_down.weight
create_tensor: loading tensor blk.26.ffn_up.weight
create_tensor: loading tensor blk.27.attn_norm.weight
create_tensor: loading tensor blk.27.attn_q.weight
create_tensor: loading tensor blk.27.attn_k.weight
create_tensor: loading tensor blk.27.attn_v.weight
create_tensor: loading tensor blk.27.attn_output.weight
create_tensor: loading tensor blk.27.attn_q.bias
create_tensor: loading tensor blk.27.attn_k.bias
create_tensor: loading tensor blk.27.attn_v.bias
create_tensor: loading tensor blk.27.ffn_norm.weight
create_tensor: loading tensor blk.27.ffn_gate.weight
create_tensor: loading tensor blk.27.ffn_down.weight
create_tensor: loading tensor blk.27.ffn_up.weight
load_tensors: tensor 'token_embd.weight' (q4_K) (and 170 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead
load_tensors:   CPU_Mapped model buffer size =  4460.45 MiB
load_tensors:   CPU_REPACK model buffer size =  2976.75 MiB
................repack: repack tensor blk.0.attn_q.weight with q4_K_8x8
repack: repack tensor blk.0.attn_k.weight with q4_K_8x8
repack: repack tensor blk.0.attn_output.weight with q4_K_8x8
repack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8
.repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8
.repack: repack tensor blk.1.attn_q.weight with q4_K_8x8
repack: repack tensor blk.1.attn_k.weight with q4_K_8x8
repack: repack tensor blk.1.attn_output.weight with q4_K_8x8
repack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8
.repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8
.repack: repack tensor blk.2.attn_q.weight with q4_K_8x8
repack: repack tensor blk.2.attn_k.weight with q4_K_8x8
repack: repack tensor blk.2.attn_output.weight with q4_K_8x8
repack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8
.repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8
.repack: repack tensor blk.3.attn_q.weight with q4_K_8x8
repack: repack tensor blk.3.attn_k.weight with q4_K_8x8
repack: repack tensor blk.3.attn_v.weight with q4_K_8x8
repack: repack tensor blk.3.attn_output.weight with q4_K_8x8
repack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8
.repack: repack tensor blk.3.ffn_down.weight with q4_K_8x8
.repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8
repack: repack tensor blk.4.attn_q.weight with q4_K_8x8
.repack: repack tensor blk.4.attn_k.weight with q4_K_8x8
repack: repack tensor blk.4.attn_v.weight with q4_K_8x8
repack: repack tensor blk.4.attn_output.weight with q4_K_8x8
repack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8
.repack: repack tensor blk.4.ffn_down.weight with q4_K_8x8
repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8
.repack: repack tensor blk.5.attn_q.weight with q4_K_8x8
repack: repack tensor blk.5.attn_k.weight with q4_K_8x8
repack: repack tensor blk.5.attn_output.weight with q4_K_8x8
.repack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8
repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8
.repack: repack tensor blk.6.attn_q.weight with q4_K_8x8
repack: repack tensor blk.6.attn_k.weight with q4_K_8x8
repack: repack tensor blk.6.attn_v.weight with q4_K_8x8
repack: repack tensor blk.6.attn_output.weight with q4_K_8x8
.repack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8
repack: repack tensor blk.6.ffn_down.weight with q4_K_8x8
.repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8
.repack: repack tensor blk.7.attn_q.weight with q4_K_8x8
repack: repack tensor blk.7.attn_k.weight with q4_K_8x8
repack: repack tensor blk.7.attn_v.weight with q4_K_8x8
repack: repack tensor blk.7.attn_output.weight with q4_K_8x8
repack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8
.repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8
.repack: repack tensor blk.8.attn_q.weight with q4_K_8x8
repack: repack tensor blk.8.attn_k.weight with q4_K_8x8
repack: repack tensor blk.8.attn_v.weight with q4_K_8x8
repack: repack tensor blk.8.attn_output.weight with q4_K_8x8
repack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8
.repack: repack tensor blk.8.ffn_down.weight with q4_K_8x8
.repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8
.repack: repack tensor blk.9.attn_q.weight with q4_K_8x8
repack: repack tensor blk.9.attn_k.weight with q4_K_8x8
repack: repack tensor blk.9.attn_output.weight with q4_K_8x8
repack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8
.repack: repack tensor blk.9.ffn_down.weight with q4_K_8x8
.repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8
.repack: repack tensor blk.10.attn_q.weight with q4_K_8x8
repack: repack tensor blk.10.attn_k.weight with q4_K_8x8
repack: repack tensor blk.10.attn_v.weight with q4_K_8x8
repack: repack tensor blk.10.attn_output.weight with q4_K_8x8
repack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8
.repack: repack tensor blk.10.ffn_down.weight with q4_K_8x8
.repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8
repack: repack tensor blk.11.attn_q.weight with q4_K_8x8
.repack: repack tensor blk.11.attn_k.weight with q4_K_8x8
repack: repack tensor blk.11.attn_output.weight with q4_K_8x8
repack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8
.repack: repack tensor blk.11.ffn_down.weight with q4_K_8x8
repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8
.repack: repack tensor blk.12.attn_q.weight with q4_K_8x8
repack: repack tensor blk.12.attn_k.weight with q4_K_8x8
repack: repack tensor blk.12.attn_v.weight with q4_K_8x8
repack: repack tensor blk.12.attn_output.weight with q4_K_8x8
.repack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8
repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8
.repack: repack tensor blk.13.attn_q.weight with q4_K_8x8
repack: repack tensor blk.13.attn_k.weight with q4_K_8x8
repack: repack tensor blk.13.attn_v.weight with q4_K_8x8
repack: repack tensor blk.13.attn_output.weight with q4_K_8x8
.repack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8
repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8
.repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8
.repack: repack tensor blk.14.attn_q.weight with q4_K_8x8
repack: repack tensor blk.14.attn_k.weight with q4_K_8x8
repack: repack tensor blk.14.attn_output.weight with q4_K_8x8
repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8
.repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8
.repack: repack tensor blk.15.attn_q.weight with q4_K_8x8
repack: repack tensor blk.15.attn_k.weight with q4_K_8x8
repack: repack tensor blk.15.attn_v.weight with q4_K_8x8
repack: repack tensor blk.15.attn_output.weight with q4_K_8x8
repack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8
.repack: repack tensor blk.15.ffn_down.weight with q4_K_8x8
.repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8
.repack: repack tensor blk.16.attn_q.weight with q4_K_8x8
repack: repack tensor blk.16.attn_k.weight with q4_K_8x8
repack: repack tensor blk.16.attn_v.weight with q4_K_8x8
repack: repack tensor blk.16.attn_output.weight with q4_K_8x8
repack: repack tensor blk.16.ffn_gate.weight with q4_K_8x8
.repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8
.repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8
.repack: repack tensor blk.17.attn_q.weight with q4_K_8x8
repack: repack tensor blk.17.attn_k.weight with q4_K_8x8
repack: repack tensor blk.17.attn_output.weight with q4_K_8x8
repack: repack tensor blk.17.ffn_gate.weight with q4_K_8x8
.repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8
.repack: repack tensor blk.18.attn_q.weight with q4_K_8x8
repack: repack tensor blk.18.attn_k.weight with q4_K_8x8
repack: repack tensor blk.18.attn_v.weight with q4_K_8x8
repack: repack tensor blk.18.attn_output.weight with q4_K_8x8
repack: repack tensor blk.18.ffn_gate.weight with q4_K_8x8
.repack: repack tensor blk.18.ffn_down.weight with q4_K_8x8
repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8
.repack: repack tensor blk.19.attn_q.weight with q4_K_8x8
repack: repack tensor blk.19.attn_k.weight with q4_K_8x8
repack: repack tensor blk.19.attn_v.weight with q4_K_8x8
.repack: repack tensor blk.19.attn_output.weight with q4_K_8x8
repack: repack tensor blk.19.ffn_gate.weight with q4_K_8x8
repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8
.repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8
.repack: repack tensor blk.20.attn_q.weight with q4_K_8x8
repack: repack tensor blk.20.attn_k.weight with q4_K_8x8
repack: repack tensor blk.20.attn_output.weight with q4_K_8x8
repack: repack tensor blk.20.ffn_gate.weight with q4_K_8x8
.repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8
.repack: repack tensor blk.21.attn_q.weight with q4_K_8x8
repack: repack tensor blk.21.attn_k.weight with q4_K_8x8
repack: repack tensor blk.21.attn_v.weight with q4_K_8x8
repack: repack tensor blk.21.attn_output.weight with q4_K_8x8
repack: repack tensor blk.21.ffn_gate.weight with q4_K_8x8
.repack: repack tensor blk.21.ffn_down.weight with q4_K_8x8
.repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8
.repack: repack tensor blk.22.attn_q.weight with q4_K_8x8
repack: repack tensor blk.22.attn_k.weight with q4_K_8x8
repack: repack tensor blk.22.attn_v.weight with q4_K_8x8
repack: repack tensor blk.22.attn_output.weight with q4_K_8x8
repack: repack tensor blk.22.ffn_gate.weight with q4_K_8x8
.repack: repack tensor blk.22.ffn_down.weight with q4_K_8x8
.repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8
.repack: repack tensor blk.23.attn_q.weight with q4_K_8x8
repack: repack tensor blk.23.attn_k.weight with q4_K_8x8
repack: repack tensor blk.23.attn_output.weight with q4_K_8x8
repack: repack tensor blk.23.ffn_gate.weight with q4_K_8x8
.repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8
.repack: repack tensor blk.24.attn_q.weight with q4_K_8x8
repack: repack tensor blk.24.attn_k.weight with q4_K_8x8
repack: repack tensor blk.24.attn_output.weight with q4_K_8x8
repack: repack tensor blk.24.ffn_gate.weight with q4_K_8x8
.repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8
.repack: repack tensor blk.25.attn_q.weight with q4_K_8x8
repack: repack tensor blk.25.attn_k.weight with q4_K_8x8
repack: repack tensor blk.25.attn_output.weight with q4_K_8x8
repack: repack tensor blk.25.ffn_gate.weight with q4_K_8x8
.repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8
.repack: repack tensor blk.26.attn_q.weight with q4_K_8x8
repack: repack tensor blk.26.attn_k.weight with q4_K_8x8
repack: repack tensor blk.26.attn_output.weight with q4_K_8x8
repack: repack tensor blk.26.ffn_gate.weight with q4_K_8x8
.repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8
.repack: repack tensor blk.27.attn_q.weight with q4_K_8x8
repack: repack tensor blk.27.attn_k.weight with q4_K_8x8
repack: repack tensor blk.27.attn_output.weight with q4_K_8x8
repack: repack tensor blk.27.ffn_gate.weight with q4_K_8x8
.repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8
.
|     *     | Alt:  98.98 m, Vel: -10.16 m/s
|     *     | Alt:  97.95 m, Vel: -10.32 m/s
|     *     | Alt:  96.90 m, Vel: -10.49 m/s
|     *     | Alt:  95.84 m, Vel: -10.65 m/s
|     *     | Alt:  94.76 m, Vel: -10.81 m/s
|     *     | Alt:  93.66 m, Vel: -10.97 m/s
|     *     | Alt:  92.55 m, Vel: -11.13 m/s
|     *     | Alt:  91.42 m, Vel: -11.30 m/s
|     *     | Alt:  90.27 m, Vel: -11.46 m/s
|     *     | Alt:  89.11 m, Vel: -11.62 m/s
|     *     | Alt:  87.93 m, Vel: -11.78 m/s
|     *     | Alt:  86.74 m, Vel: -11.94 m/s
|     *     | Alt:  85.53 m, Vel: -12.11 m/s
|     *     | Alt:  84.30 m, Vel: -12.27 m/s
|     *     | Alt:  83.06 m, Vel: -12.43 m/s
|     *     | Alt:  81.80 m, Vel: -12.59 m/s
|     *     | Alt:  80.52 m, Vel: -12.75 m/s
|     *     | Alt:  79.23 m, Vel: -12.92 m/s
|     *     | Alt:  77.92 m, Vel: -13.08 m/s
|     *     | Alt:  76.60 m, Vel: -13.24 m/s
|     *     | Alt:  75.26 m, Vel: -13.40 m/s
|     *     | Alt:  73.90 m, Vel: -13.56 m/s
|     *     | Alt:  72.53 m, Vel: -13.73 m/s
|     *     | Alt:  71.14 m, Vel: -13.89 m/s
|     *     | Alt:  69.74 m, Vel: -14.05 m/s
|     *     | Alt:  68.31 m, Vel: -14.21 m/s
|     *     | Alt:  66.88 m, Vel: -14.37 m/s
|     *     | Alt:  65.42 m, Vel: -14.54 m/s
|     *     | Alt:  63.95 m, Vel: -14.70 m/s
|     *     | Alt:  62.47 m, Vel: -14.86 m/s
|     *     | Alt:  60.96 m, Vel: -15.02 m/s
|     *     | Alt:  59.45 m, Vel: -15.18 m/s
|     *     | Alt:  57.91 m, Vel: -15.35 m/s
|     *     | Alt:  56.36 m, Vel: -15.51 m/s
|     *     | Alt:  54.79 m, Vel: -15.67 m/s
|     *     | Alt:  53.21 m, Vel: -15.83 m/s
|     *     | Alt:  51.61 m, Vel: -15.99 m/s
|     *     | Alt:  50.00 m, Vel: -16.16 m/s
|     *     | Alt:  48.36 m, Vel: -16.32 m/s
|     *     | Alt:  46.72 m, Vel: -16.48 m/s
|     *     | Alt:  45.05 m, Vel: -16.64 m/s
|     *     | Alt:  43.37 m, Vel: -16.80 m/s
|     *     | Alt:  41.67 m, Vel: -16.97 m/s
|     *     | Alt:  39.96 m, Vel: -17.13 m/s
|     *     | Alt:  38.23 m, Vel: -17.29 m/s
|     *     | Alt:  36.49 m, Vel: -17.45 m/s
|     *     | Alt:  34.73 m, Vel: -17.61 m/s
|     *     | Alt:  32.95 m, Vel: -17.78 m/s
|     *     | Alt:  31.16 m, Vel: -17.94 m/s
|     *     | Alt:  29.35 m, Vel: -18.10 m/s
|     *     | Alt:  27.52 m, Vel: -18.26 m/s
|     *     | Alt:  25.68 m, Vel: -18.42 m/s
|     *     | Alt:  23.82 m, Vel: -18.59 m/s
|     *     | Alt:  21.94 m, Vel: -18.75 m/s
|     *     | Alt:  20.05 m, Vel: -18.91 m/s
|     *     | Alt:  18.14 m, Vel: -19.07 m/s
|     *     | Alt:  16.22 m, Vel: -19.23 m/s
|     *     | Alt:  14.28 m, Vel: -19.40 m/s
|     *     | Alt:  12.33 m, Vel: -19.56 m/s
|     *     | Alt:  10.35 m, Vel: -19.72 m/s
|     *     | Alt:   8.37 m, Vel: -19.88 m/s
|     *     | Alt:   6.36 m, Vel: -20.04 m/s
|     *     | Alt:   4.34 m, Vel: -20.21 m/s
|     *     | Alt:   2.30 m, Vel: -20.37 m/s
|     *     | Alt:   0.25 m, Vel: -20.53 m/s
|     *     | Alt:  -1.82 m, Vel: -20.69 m/s
[System 2] CRASH IMMINENT! Violation detected (Attempt 1).
[System 2] Cache miss. Querying Gemini for a fix...
[LlamaCpp] Querying model...
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2304
llama_context: n_ctx_seq     = 2304
llama_context: n_batch       = 1090
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (2304) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:        CPU  output buffer size =     0.58 MiB
llama_kv_cache: layer   0: dev = CPU
llama_kv_cache: layer   1: dev = CPU
llama_kv_cache: layer   2: dev = CPU
llama_kv_cache: layer   3: dev = CPU
llama_kv_cache: layer   4: dev = CPU
llama_kv_cache: layer   5: dev = CPU
llama_kv_cache: layer   6: dev = CPU
llama_kv_cache: layer   7: dev = CPU
llama_kv_cache: layer   8: dev = CPU
llama_kv_cache: layer   9: dev = CPU
llama_kv_cache: layer  10: dev = CPU
llama_kv_cache: layer  11: dev = CPU
llama_kv_cache: layer  12: dev = CPU
llama_kv_cache: layer  13: dev = CPU
llama_kv_cache: layer  14: dev = CPU
llama_kv_cache: layer  15: dev = CPU
llama_kv_cache: layer  16: dev = CPU
llama_kv_cache: layer  17: dev = CPU
llama_kv_cache: layer  18: dev = CPU
llama_kv_cache: layer  19: dev = CPU
llama_kv_cache: layer  20: dev = CPU
llama_kv_cache: layer  21: dev = CPU
llama_kv_cache: layer  22: dev = CPU
llama_kv_cache: layer  23: dev = CPU
llama_kv_cache: layer  24: dev = CPU
llama_kv_cache: layer  25: dev = CPU
llama_kv_cache: layer  26: dev = CPU
llama_kv_cache: layer  27: dev = CPU
llama_kv_cache:        CPU KV buffer size =   126.00 MiB
llama_kv_cache: size =  126.00 MiB (  2304 cells,  28 layers,  1/1 seqs), K (f16):   63.00 MiB, V (f16):   63.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 1
sched_reserve: reserving ...
sched_reserve: max_nodes = 2712
sched_reserve: reserving full memory module
sched_reserve: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 1
graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1
sched_reserve: Flash Attention was auto, set to enabled
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512
sched_reserve:        CPU compute buffer size =   311.00 MiB
sched_reserve: graph nodes  = 959
sched_reserve: graph splits = 1
sched_reserve: reserve took 2.16 ms, sched copies = 1
[LlamaCpp] Inference finished in 57.3737s (13.6648 tokens/sec)
[Runtime] Model returned 1600 bytes.
[System 2] Hot-swapping fixed code...
[StrategyCache] Saved to /usr/local/google/home/antoniopaulino/.neurojit/cache.json
[System 2] Success! Logic updated. Unwinding stack for restart...
[NeuroJIT] Recovery point reached. Restarting simulation...
|     *     | Alt:  98.98 m, Vel: -10.16 m/s
|     *     | Alt:  97.95 m, Vel: -10.32 m/s
|     *     | Alt:  96.90 m, Vel: -10.49 m/s
|     *     | Alt:  95.84 m, Vel: -10.65 m/s
|     *     | Alt:  94.76 m, Vel: -10.81 m/s
|     *     | Alt:  93.66 m, Vel: -10.97 m/s
|     *     | Alt:  92.55 m, Vel: -11.13 m/s
|     *     | Alt:  91.42 m, Vel: -11.30 m/s
|     *     | Alt:  90.27 m, Vel: -11.46 m/s
|     *     | Alt:  89.11 m, Vel: -11.62 m/s
|     *     | Alt:  87.93 m, Vel: -11.78 m/s
|     *     | Alt:  86.74 m, Vel: -11.94 m/s
|     *     | Alt:  85.53 m, Vel: -12.11 m/s
|     *     | Alt:  84.30 m, Vel: -12.27 m/s
|     *     | Alt:  83.06 m, Vel: -12.43 m/s
|     *     | Alt:  81.80 m, Vel: -12.59 m/s
|     *     | Alt:  80.52 m, Vel: -12.75 m/s
|     *     | Alt:  79.23 m, Vel: -12.92 m/s
|     *     | Alt:  77.92 m, Vel: -13.08 m/s
|     *     | Alt:  76.60 m, Vel: -13.24 m/s
|     *     | Alt:  75.26 m, Vel: -13.40 m/s
|     *     | Alt:  73.90 m, Vel: -13.56 m/s
|     *     | Alt:  72.53 m, Vel: -13.73 m/s
|     *     | Alt:  71.14 m, Vel: -13.89 m/s
|     *     | Alt:  69.74 m, Vel: -14.05 m/s
|     *     | Alt:  68.31 m, Vel: -14.21 m/s
|     *     | Alt:  66.88 m, Vel: -14.37 m/s
|     *     | Alt:  65.42 m, Vel: -14.54 m/s
|     *     | Alt:  63.95 m, Vel: -14.70 m/s
|     *     | Alt:  62.47 m, Vel: -14.86 m/s
|     *     | Alt:  60.96 m, Vel: -15.02 m/s
|     *     | Alt:  59.45 m, Vel: -15.18 m/s
|     *     | Alt:  57.91 m, Vel: -15.35 m/s
|     *     | Alt:  56.36 m, Vel: -15.51 m/s
|     *     | Alt:  54.79 m, Vel: -15.67 m/s
|     *     | Alt:  53.21 m, Vel: -15.83 m/s
|     *     | Alt:  51.61 m, Vel: -15.99 m/s
|     *     | Alt:  50.00 m, Vel: -16.16 m/s
|     *     | Alt:  48.36 m, Vel: -16.32 m/s
|     *     | Alt:  46.72 m, Vel: -16.48 m/s
|     *     | Alt:  45.05 m, Vel: -16.64 m/s
|     *     | Alt:  43.37 m, Vel: -16.80 m/s
|     *     | Alt:  41.67 m, Vel: -16.97 m/s
|     *     | Alt:  39.96 m, Vel: -17.13 m/s
|     *     | Alt:  38.23 m, Vel: -17.29 m/s
|     *     | Alt:  36.49 m, Vel: -17.45 m/s
|     *     | Alt:  34.73 m, Vel: -17.61 m/s
|     *     | Alt:  32.95 m, Vel: -17.78 m/s
|     *     | Alt:  31.16 m, Vel: -17.94 m/s
|     *     | Alt:  29.35 m, Vel: -18.10 m/s
|     *     | Alt:  27.52 m, Vel: -18.26 m/s
|     *     | Alt:  25.68 m, Vel: -18.42 m/s
|     *     | Alt:  23.82 m, Vel: -18.59 m/s
|     *     | Alt:  21.94 m, Vel: -18.75 m/s
|     *     | Alt:  20.05 m, Vel: -18.91 m/s
|     *     | Alt:  18.14 m, Vel: -19.07 m/s
|     *     | Alt:  16.22 m, Vel: -19.23 m/s
|     *     | Alt:  14.28 m, Vel: -19.40 m/s
|     *     | Alt:  12.33 m, Vel: -19.56 m/s
|     *     | Alt:  10.35 m, Vel: -19.72 m/s
|     *     | Alt:   8.37 m, Vel: -19.88 m/s
|     *     | Alt:   6.36 m, Vel: -20.04 m/s
|     *     | Alt:   4.34 m, Vel: -20.21 m/s
|     *     | Alt:   2.30 m, Vel: -20.37 m/s
|     *     | Alt:   0.25 m, Vel: -20.53 m/s
|     *     | Alt:  -1.82 m, Vel: -20.69 m/s
[System 2] CRASH IMMINENT! Violation detected (Attempt 2).
[System 2] Cache hit! Applying fix immediately...
[System 2] Success! Logic updated. Unwinding stack for restart...
[NeuroJIT] Recovery point reached. Restarting simulation...
|     *     | Alt:  98.98 m, Vel: -10.16 m/s
|     *     | Alt:  97.95 m, Vel: -10.32 m/s
|     *     | Alt:  96.90 m, Vel: -10.49 m/s
|     *     | Alt:  95.84 m, Vel: -10.65 m/s
|     *     | Alt:  94.76 m, Vel: -10.81 m/s
|     *     | Alt:  93.66 m, Vel: -10.97 m/s
|     *     | Alt:  92.55 m, Vel: -11.13 m/s
|     *     | Alt:  91.42 m, Vel: -11.30 m/s
|     *     | Alt:  90.27 m, Vel: -11.46 m/s
|     *     | Alt:  89.11 m, Vel: -11.62 m/s
|     *     | Alt:  87.93 m, Vel: -11.78 m/s
|     *     | Alt:  86.74 m, Vel: -11.94 m/s
|     *     | Alt:  85.53 m, Vel: -12.11 m/s
|     *     | Alt:  84.30 m, Vel: -12.27 m/s
|     *     | Alt:  83.06 m, Vel: -12.43 m/s
|     *     | Alt:  81.80 m, Vel: -12.59 m/s
|     *     | Alt:  80.52 m, Vel: -12.75 m/s
|     *     | Alt:  79.23 m, Vel: -12.92 m/s
|     *     | Alt:  77.92 m, Vel: -13.08 m/s
|     *     | Alt:  76.60 m, Vel: -13.24 m/s
|     *     | Alt:  75.26 m, Vel: -13.40 m/s
|     *     | Alt:  73.90 m, Vel: -13.56 m/s
|     *     | Alt:  72.53 m, Vel: -13.73 m/s
|     *     | Alt:  71.14 m, Vel: -13.89 m/s
|     *     | Alt:  69.74 m, Vel: -14.05 m/s
|     *     | Alt:  68.31 m, Vel: -14.21 m/s
|     *     | Alt:  66.88 m, Vel: -14.37 m/s
|     *     | Alt:  65.42 m, Vel: -14.54 m/s
|     *     | Alt:  63.95 m, Vel: -14.70 m/s
|     *     | Alt:  62.47 m, Vel: -14.86 m/s
|     *     | Alt:  60.96 m, Vel: -15.02 m/s
|     *     | Alt:  59.45 m, Vel: -15.18 m/s
|     *     | Alt:  57.91 m, Vel: -15.35 m/s
|     *     | Alt:  56.36 m, Vel: -15.51 m/s
|     *     | Alt:  54.79 m, Vel: -15.67 m/s
|     *     | Alt:  53.21 m, Vel: -15.83 m/s
|     *     | Alt:  51.61 m, Vel: -15.99 m/s
|     *     | Alt:  50.00 m, Vel: -16.16 m/s
|     *     | Alt:  48.36 m, Vel: -16.32 m/s
|     *     | Alt:  46.72 m, Vel: -16.48 m/s
|     *     | Alt:  45.05 m, Vel: -16.64 m/s
|     *     | Alt:  43.37 m, Vel: -16.80 m/s
|     *     | Alt:  41.67 m, Vel: -16.97 m/s
|     *     | Alt:  39.96 m, Vel: -17.13 m/s
|     *     | Alt:  38.23 m, Vel: -17.29 m/s
|     *     | Alt:  36.49 m, Vel: -17.45 m/s
|     *     | Alt:  34.73 m, Vel: -17.61 m/s
|     *     | Alt:  32.95 m, Vel: -17.78 m/s
|     *     | Alt:  31.16 m, Vel: -17.94 m/s
|     *     | Alt:  29.35 m, Vel: -18.10 m/s
|     *     | Alt:  27.52 m, Vel: -18.26 m/s
|     *     | Alt:  25.68 m, Vel: -18.42 m/s
|     *     | Alt:  23.82 m, Vel: -18.59 m/s
|     *     | Alt:  21.94 m, Vel: -18.75 m/s
|     *     | Alt:  20.05 m, Vel: -18.91 m/s
|     *     | Alt:  18.14 m, Vel: -19.07 m/s
|     *     | Alt:  16.22 m, Vel: -19.23 m/s
|     *     | Alt:  14.28 m, Vel: -19.40 m/s
|     *     | Alt:  12.33 m, Vel: -19.56 m/s
|     *     | Alt:  10.35 m, Vel: -19.72 m/s
|     *     | Alt:   8.37 m, Vel: -19.88 m/s
|     *     | Alt:   6.36 m, Vel: -20.04 m/s
|     *     | Alt:   4.34 m, Vel: -20.21 m/s
|     *     | Alt:   2.30 m, Vel: -20.37 m/s
|     *     | Alt:   0.25 m, Vel: -20.53 m/s
|     *     | Alt:  -1.82 m, Vel: -20.69 m/s
[System 2] CRASH IMMINENT! Violation detected (Attempt 3).
[System 2] Cache hit! Applying fix immediately...
[System 2] Success! Logic updated. Unwinding stack for restart...
[NeuroJIT] Recovery point reached. Restarting simulation...
|     *     | Alt:  98.98 m, Vel: -10.16 m/s
|     *     | Alt:  97.95 m, Vel: -10.32 m/s
|     *     | Alt:  96.90 m, Vel: -10.49 m/s
|     *     | Alt:  95.84 m, Vel: -10.65 m/s
|     *     | Alt:  94.76 m, Vel: -10.81 m/s
|     *     | Alt:  93.66 m, Vel: -10.97 m/s
|     *     | Alt:  92.55 m, Vel: -11.13 m/s
|     *     | Alt:  91.42 m, Vel: -11.30 m/s
|     *     | Alt:  90.27 m, Vel: -11.46 m/s
|     *     | Alt:  89.11 m, Vel: -11.62 m/s
|     *     | Alt:  87.93 m, Vel: -11.78 m/s
|     *     | Alt:  86.74 m, Vel: -11.94 m/s
|     *     | Alt:  85.53 m, Vel: -12.11 m/s
|     *     | Alt:  84.30 m, Vel: -12.27 m/s
|     *     | Alt:  83.06 m, Vel: -12.43 m/s
|     *     | Alt:  81.80 m, Vel: -12.59 m/s
|     *     | Alt:  80.52 m, Vel: -12.75 m/s
|     *     | Alt:  79.23 m, Vel: -12.92 m/s
|     *     | Alt:  77.92 m, Vel: -13.08 m/s
|     *     | Alt:  76.60 m, Vel: -13.24 m/s
|     *     | Alt:  75.26 m, Vel: -13.40 m/s
|     *     | Alt:  73.90 m, Vel: -13.56 m/s
|     *     | Alt:  72.53 m, Vel: -13.73 m/s
|     *     | Alt:  71.14 m, Vel: -13.89 m/s
|     *     | Alt:  69.74 m, Vel: -14.05 m/s
|     *     | Alt:  68.31 m, Vel: -14.21 m/s
|     *     | Alt:  66.88 m, Vel: -14.37 m/s
|     *     | Alt:  65.42 m, Vel: -14.54 m/s
|     *     | Alt:  63.95 m, Vel: -14.70 m/s
|     *     | Alt:  62.47 m, Vel: -14.86 m/s
|     *     | Alt:  60.96 m, Vel: -15.02 m/s
|     *     | Alt:  59.45 m, Vel: -15.18 m/s
|     *     | Alt:  57.91 m, Vel: -15.35 m/s
|     *     | Alt:  56.36 m, Vel: -15.51 m/s
|     *     | Alt:  54.79 m, Vel: -15.67 m/s
|     *     | Alt:  53.21 m, Vel: -15.83 m/s
|     *     | Alt:  51.61 m, Vel: -15.99 m/s
|     *     | Alt:  50.00 m, Vel: -16.16 m/s
|     *     | Alt:  48.36 m, Vel: -16.32 m/s
|     *     | Alt:  46.72 m, Vel: -16.48 m/s
|     *     | Alt:  45.05 m, Vel: -16.64 m/s
|     *     | Alt:  43.37 m, Vel: -16.80 m/s
|     *     | Alt:  41.67 m, Vel: -16.97 m/s
|     *     | Alt:  39.96 m, Vel: -17.13 m/s
|     *     | Alt:  38.23 m, Vel: -17.29 m/s
|     *     | Alt:  36.49 m, Vel: -17.45 m/s
|     *     | Alt:  34.73 m, Vel: -17.61 m/s
|     *     | Alt:  32.95 m, Vel: -17.78 m/s
|     *     | Alt:  31.16 m, Vel: -17.94 m/s
|     *     | Alt:  29.35 m, Vel: -18.10 m/s
|     *     | Alt:  27.52 m, Vel: -18.26 m/s
|     *     | Alt:  25.68 m, Vel: -18.42 m/s
|     *     | Alt:  23.82 m, Vel: -18.59 m/s
|     *     | Alt:  21.94 m, Vel: -18.75 m/s
|     *     | Alt:  20.05 m, Vel: -18.91 m/s
|     *     | Alt:  18.14 m, Vel: -19.07 m/s
|     *     | Alt:  16.22 m, Vel: -19.23 m/s
|     *     | Alt:  14.28 m, Vel: -19.40 m/s
|     *     | Alt:  12.33 m, Vel: -19.56 m/s
|     *     | Alt:  10.35 m, Vel: -19.72 m/s
|     *     | Alt:   8.37 m, Vel: -19.88 m/s
|     *     | Alt:   6.36 m, Vel: -20.04 m/s
|     *     | Alt:   4.34 m, Vel: -20.21 m/s
|     *     | Alt:   2.30 m, Vel: -20.37 m/s
|     *     | Alt:   0.25 m, Vel: -20.53 m/s
|     *     | Alt:  -1.82 m, Vel: -20.69 m/s
[System 2] Self-healing attempted but failed to prevent crash. Manual intervention required.
[NeuroJIT] Using Runner: llama
~llama_context:        CPU compute buffer size is 311.0020 MiB, matches expectation of 311.0020 MiB
