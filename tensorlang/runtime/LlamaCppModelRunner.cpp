#include "tensorlang/Runtime/ModelRunner.h"
#include "tensorlang/Runtime/JitContext.h"
#include "llama.h"
#include <iostream>
#include <vector>
#include <string>
#include <sstream>
#include <fstream>
#include <mutex>
#include <sys/stat.h>

namespace mlir {
namespace tensorlang {

/// Loads a .gbnf grammar file from disk.
/// Returns empty string on failure (grammar is optional — LLM still works
/// without it, just less constrained).
static std::string loadGrammarFile(const std::string& path) {
  std::ifstream f(path);
  if (!f.is_open()) {
    fprintf(stderr, "[LLM] WARNING: Grammar file not found: %s\n",
            path.c_str());
    return "";
  }
  std::ostringstream ss;
  ss << f.rdbuf();
  return ss.str();
}

class LlamaCppModelRunner : public ModelRunner {
public:
  LlamaCppModelRunner() { ggml_backend_load_all(); }

  ~LlamaCppModelRunner() {
    // Models are freed here — contexts are freed per-query (Fresh Context pattern)
    if (brainModel_)  llama_model_free(brainModel_);
    if (muscleModel_) llama_model_free(muscleModel_);
  }

  int load(const std::string& /*modelPath*/) override {
    // Model paths are hardcoded by convention.
    // The --model CLI arg is used as a hint but we load the full pair.
    const std::string base = "tensorlang/runtime/models/";
    const std::string brainPath  = base + "deepseek-r1-32b-q4_k_m.gguf";
    const std::string musclePath = base + "qwen2.5-coder-7b-instruct-q4_k_m.gguf";

    llama_model_params mparams = llama_model_default_params();
    mparams.n_gpu_layers = 0; // CPU only

    printf("[LLM] Loading muscle model: %s\n", musclePath.c_str());
    muscleModel_ = llama_model_load_from_file(musclePath.c_str(), mparams);

    if (!muscleModel_) {
      fprintf(stderr, "[LLM] ERROR: Failed to load muscle model\n");
      return -1;
    }

    // Brain is optional — used only for LLM oracle mutation in the GA
    printf("[LLM] Loading brain model: %s\n", brainPath.c_str());
    brainModel_ = llama_model_load_from_file(brainPath.c_str(), mparams);
    if (!brainModel_) {
      fprintf(stderr, "[LLM] WARNING: Brain model not loaded. "
                      "GA will use standard mutation only.\n");
    }

    // Load the GBNF grammar that constrains output to ControlStrategy JSON
    grammarStr_ = loadGrammarFile(
        "tensorlang/runtime/grammars/control_strategy.gbnf");

    printf("[LLM] Ready. Grammar: %s\n",
           grammarStr_.empty() ? "DISABLED" : "ACTIVE");
    return 0;
  }

  /// Query the model. The prompt should ask for optimization parameters.
  /// Returns JSON string matching ControlStrategy schema.
  /// NEVER returns raw MLIR. Raw MLIR is generated by MLIRTemplates.h.
  std::string query(const std::string& prompt) override {
    std::lock_guard<std::mutex> lock(queryMutex_);

    // Use brain for strategic reasoning, muscle for JSON output
    llama_model* model = brainModel_ ? brainModel_ : muscleModel_;
    llama_context* ctx = createContext(model, 1024);
    if (!ctx) return buildFallbackJSON();

    std::string formatted = formatPrompt(prompt);
    std::string raw = runInference(ctx, model, formatted,
                                   /*n_predict=*/256,
                                   /*use_grammar=*/true);
    llama_free(ctx);

    printf("[LLM] Raw response: %s\n", raw.c_str());
    return extractJSON(raw);
  }

private:
  std::mutex queryMutex_;
  llama_model* brainModel_  = nullptr;
  llama_model* muscleModel_ = nullptr;
  std::string grammarStr_;

  llama_context* createContext(llama_model* m, int n_ctx) {
    if (!m) return nullptr;
    llama_context_params p = llama_context_default_params();
    p.n_ctx           = n_ctx;
    p.n_threads       = 64;
    p.n_threads_batch = 64;
    return llama_init_from_model(m, p);
  }

  std::string formatPrompt(const std::string& user_content) {
    // ChatML format — works for both Qwen and DeepSeek models
    std::ostringstream ss;
    ss << "<|im_start|>system\n"
       << "You are a control systems optimization expert. "
       << "You MUST respond with ONLY a JSON object. "
       << "No prose, no explanation, no markdown. "
       << "The JSON must have exactly these keys: "
       << "kp, ki, kd, target_velocity, thrust_clamp_max. "
       << "All values are floating point numbers.\n"
       << "<|im_end|>\n"
       << "<|im_start|>user\n"
       << user_content << "\n"
       << "<|im_end|>\n"
       << "<|im_start|>assistant\n"
       << "{"; // Prime the model to start JSON immediately
    return ss.str();
  }

  std::string runInference(llama_context* ctx,
                            llama_model* model,
                            const std::string& prompt,
                            int n_predict,
                            bool use_grammar) {
    const llama_vocab* vocab = llama_model_get_vocab(model);

    // Tokenize
    int n_prompt = -llama_tokenize(vocab, prompt.c_str(), prompt.size(),
                                   nullptr, 0, false, true);
    std::vector<llama_token> tokens(n_prompt);
    llama_tokenize(vocab, prompt.c_str(), prompt.size(),
                   tokens.data(), tokens.size(), false, true);

    llama_batch batch = llama_batch_get_one(tokens.data(), tokens.size());
    if (llama_decode(ctx, batch)) return "";

    // Sampler chain
    auto sparams = llama_sampler_chain_default_params();
    llama_sampler* smpl = llama_sampler_chain_init(sparams);

    // Add grammar constraint if available and requested
    if (use_grammar && !grammarStr_.empty()) {
      llama_sampler_chain_add(
          smpl,
          llama_sampler_init_grammar(vocab, grammarStr_.c_str(), "root"));
    }

    llama_sampler_chain_add(smpl, llama_sampler_init_greedy());

    std::ostringstream ss;
    ss << "{"; // We primed with "{" in the prompt, so start result with it
    llama_token id;

    for (int i = 0; i < n_predict; i++) {
      id = llama_sampler_sample(smpl, ctx, -1);
      if (llama_vocab_is_eog(vocab, id)) break;

      char buf[256];
      int n = llama_token_to_piece(vocab, id, buf, sizeof(buf), 0, true);
      std::string piece(buf, n);
      ss << piece;

      batch = llama_batch_get_one(&id, 1);
      if (llama_decode(ctx, batch)) break;
    }

    llama_sampler_free(smpl);
    return ss.str();
  }

  /// Extracts the JSON object from the model response.
  /// The GBNF grammar should ensure this is already clean, but we
  /// add a safety extraction layer for robustness.
  std::string extractJSON(const std::string& raw) {
    size_t start = raw.find('{');
    if (start == std::string::npos) return buildFallbackJSON();

    int depth = 0;
    for (size_t i = start; i < raw.size(); i++) {
      if (raw[i] == '{') depth++;
      else if (raw[i] == '}') {
        depth--;
        if (depth == 0) return raw.substr(start, i - start + 1);
      }
    }
    return buildFallbackJSON(); // Incomplete JSON — use safe defaults
  }

  /// Returns safe default JSON if parsing fails.
  /// This ensures the system never crashes due to LLM output issues.
  std::string buildFallbackJSON() {
    return R"({"kp":1.5,"ki":0.0,"kd":1.0,"target_velocity":-1.0,"thrust_clamp_max":4.0})";
  }
};

std::unique_ptr<ModelRunner> createLlamaCppModelRunner() {
  return std::make_unique<LlamaCppModelRunner>();
}

} // namespace tensorlang
} // namespace mlir